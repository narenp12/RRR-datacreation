{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0709a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602580d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_result = pd.read_csv(\"docs_with_bert_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d169007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_id  id                title                                                                                              summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             keyphrases                                                                                                                concepts_found                                                          topic_prob\n",
       "-1        abs-0704.0047v1   Intelligent location of simultaneously active acoustic emission sources:\\n  Part I                 The intelligent acoustic emission locator is described in Part I, while Part\\nII discusses blind source separation, time delay estimation and location of two\\nsimultaneously active continuous acoustic emission sources.\\n  The location of acoustic emission on complicated aircraft frame structures is\\na difficult problem of non-destructive testing. This article describes an\\nintelligent acoustic emission source locator. The intelligent locator comprises\\na sensor antenna and a general regression neural network, which solves the\\nlocation problem based on learning from examples. Locator performance was\\ntested on different test specimens. Tests have shown that the accuracy of\\nlocation depends on sound velocity and attenuation in the specimen, the\\ndimensions of the tested area, and the properties of stored data. The location\\naccuracy achieved by the intelligent locator is comparable to that obtained by\\nthe conventional triangulation method, while the applicability of the\\nintelligent locator is more general since analysis of sonic ray paths is\\navoided. This is a promising method for non-destructive testing of aircraft\\nframe structures by the acoustic emission method.                                                                                                                                                                                                                                                                                                                                                                                                                                                 [('locator', 0.3059), ('acoustic', 0.2778), ('sensor', 0.2712), ('aircraft', 0.2631), ('emission', 0.2082)]               ['locator', 'acoustic', 'sensor', 'aircraft', 'emission']               0.0           1\n",
       "          abs-0704.1394v1   Calculating Valid Domains for BDD-Based Interactive Configuration                                  In these notes we formally describe the functionality of Calculating Valid\\nDomains from the BDD representing the solution space of valid configurations.\\nThe formalization is largely based on the CLab configuration framework.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [('formalization', 0.3637), ('configurations', 0.3502), ('bdd', 0.3428), ('domains', 0.3302), ('clab', 0.3071)]           ['formalization', 'configurations', 'bdd', 'domains', 'clab']           0.0           1\n",
       "          abs-0704.2010v2   A study of structural properties on profiles HMMs                                                  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\\nuseful tool in the detection of the remote homologue protein families.\\nUnfortunately, their performance is not always satisfactory when proteins are\\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\\nand tool that tries to improve pHMM performance by using structural information\\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\\nEach pHMM is constructed by weighting each residue in an aligned protein\\naccording to a specific structural property of the residue. Properties used\\nwere primary, secondary and tertiary structures, accessibility and packing.\\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\\nstructural aligner to align the training set proteins. Then, we performed two\\nsets of experiments. In a first experiment, we compared structure weighted\\nmodels against standard pHMMs and against each other. In a second experiment,\\nwe compared the voting model against individual pHMMs. We compare method\\nperformance through ROC curves and through Precision/Recall curves, and assess\\nsignificance through the paired two tailed t-test. Our results show significant\\nperformance improvements of all structurally weighted models over default\\nHMMER, and a significant improvement in sensitivity of the combined models over\\nboth the original model and the structurally weighted models.  [('proteins', 0.4844), ('protein', 0.459), ('markov', 0.2676), ('models', 0.2611), ('phmms', 0.2587)]                     ['proteins', 'protein', 'markov', 'models', 'phmms']                    0.0           1\n",
       "          abs-0704.2083v1   Introduction to Arabic Speech Recognition Using CMUSphinx System                                   In this paper Arabic was investigated from the speech recognition problem\\npoint of view. We propose a novel approach to build an Arabic Automated Speech\\nRecognition System (ASR). This system is based on the open source CMU Sphinx-4,\\nfrom the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\\nspeaker-independent, continuous speech recognition system based on discrete\\nHidden Markov Models (HMMs). We build a model using utilities from the\\nOpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\\nsystem to Arabic voice recognition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [('arabic', 0.5311), ('sphinx', 0.4906), ('voice', 0.3455), ('markov', 0.3449), ('speech', 0.3075)]                       ['arabic', 'sphinx', 'voice', 'markov', 'speech']                       0.0           1\n",
       "          abs-0704.2201v1   Arabic Speech Recognition System using CMU-Sphinx4                                                 In this paper we present the creation of an Arabic version of Automated\\nSpeech Recognition System (ASR). This system is based on the open source\\nSphinx-4, from the Carnegie Mellon University. Which is a speech recognition\\nsystem based on discrete hidden Markov models (HMMs). We investigate the\\nchanges that must be made to the model to adapt Arabic voice recognition.\\n  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,\\nCMUSphinx-4, Artificial intelligence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [('arabic', 0.5313), ('sphinx', 0.4505), ('markov', 0.3549), ('voice', 0.348), ('speech', 0.3003)]                        ['arabic', 'sphinx', 'markov', 'voice', 'speech']                       0.0           1\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ..\n",
       " 1136     abs-2104.13486v2  Efficient Pre-trained Features and Recurrent Pseudo-Labeling in\\n  Unsupervised Domain Adaptation  Domain adaptation (DA) mitigates the domain shift problem when transferring\\nknowledge from one annotated domain to another similar but different unlabeled\\ndomain. However, existing models often utilize one of the ImageNet models as\\nthe backbone without exploring others, and fine-tuning or retraining the\\nbackbone ImageNet model is also time-consuming. Moreover, pseudo-labeling has\\nbeen used to improve the performance in the target domain, while how to\\ngenerate confident pseudo labels and explicitly align domain distributions has\\nnot been well addressed. In this paper, we show how to efficiently opt for the\\nbest pre-trained features from seventeen well-known ImageNet models in\\nunsupervised DA problems. In addition, we propose a recurrent pseudo-labeling\\nmodel using the best pre-trained features (termed PRPL) to improve\\nclassification performance. To show the effectiveness of PRPL, we evaluate it\\non three benchmark datasets, Office+Caltech-10, Office-31, and Office-Home.\\nExtensive experiments show that our model reduces computation time and boosts\\nthe mean accuracy to 98.1%, 92.4%, and 81.2%, respectively, substantially\\noutperforming the state of the art.                                                                                                                                                                                                                                                                                                                                                                                                                                                   [('imagenet', 0.4538), ('labeling', 0.3223), ('adaptation', 0.2981), ('classification', 0.2897), ('retraining', 0.2757)]  ['imagenet', 'labeling', 'adaptation', 'classification', 'retraining']  1.0           1\n",
       "          abs-2106.14999v1  Test-Time Adaptation to Distribution Shift by Confidence Maximization\\n  and Input Transformation  Deep neural networks often exhibit poor performance on data that is unlikely\\nunder the train-time data distribution, for instance data affected by\\ncorruptions. Previous works demonstrate that test-time adaptation to data\\nshift, for instance using entropy minimization, effectively improves\\nperformance on such shifted distributions. This paper focuses on the fully\\ntest-time adaptation setting, where only unlabeled data from the target\\ndistribution is required. This allows adapting arbitrary pretrained networks.\\nSpecifically, we propose a novel loss that improves test-time adaptation by\\naddressing both premature convergence and instability of entropy minimization.\\nThis is achieved by replacing the entropy by a non-saturating surrogate and\\nadding a diversity regularizer based on batch-wise entropy maximization that\\nprevents convergence to trivial collapsed solutions. Moreover, we propose to\\nprepend an input transformation module to the network that can partially undo\\ntest-time distribution shifts. Surprisingly, this preprocessing can be learned\\nsolely using the fully test-time adaptation loss in an end-to-end fashion\\nwithout any target domain labels or source domain data. We show that our\\napproach outperforms previous work in improving the robustness of publicly\\navailable pretrained image classifiers to common corruptions on such\\nchallenging benchmarks as ImageNet-C.                                                                                                                                                                                                                         [('imagenet', 0.3513), ('adaptation', 0.3033), ('adapting', 0.2922), ('classifiers', 0.2706), ('neural', 0.2682)]         ['imagenet', 'adaptation', 'adapting', 'classifiers', 'neural']         1.0           1\n",
       "          abs-2208.07736v2  Introducing Intermediate Domains for Effective Self-Training during\\n  Test-Time                   Experiencing domain shifts during test-time is nearly inevitable in practice\\nand likely results in a severe performance degradation. To overcome this issue,\\ntest-time adaptation continues to update the initial source model during\\ndeployment. A promising direction are methods based on self-training which have\\nbeen shown to be well suited for gradual domain adaptation, since reliable\\npseudo-labels can be provided. In this work, we address two problems that exist\\nwhen applying self-training in the setting of test-time adaptation. First,\\nadapting a model to long test sequences that contain multiple domains can lead\\nto error accumulation. Second, naturally, not all shifts are gradual in\\npractice. To tackle these challenges, we introduce GTTA. By creating artificial\\nintermediate domains that divide the current domain shift into a more gradual\\none, effective self-training through high quality pseudo-labels can be\\nperformed. To create the intermediate domains, we propose two independent\\nvariations: mixup and light-weight style transfer. We demonstrate the\\neffectiveness of our approach on the continual and gradual corruption\\nbenchmarks, as well as ImageNet-R. To further investigate gradual shifts in the\\ncontext of urban scene segmentation, we publish a new benchmark: CarlaTTA. It\\nenables the exploration of several non-stationary domain shifts.                                                                                                                                                                                                                                                       [('imagenet', 0.426), ('adaptation', 0.3548), ('adapting', 0.3442), ('benchmark', 0.3208), ('training', 0.2908)]          ['imagenet', 'adaptation', 'adapting', 'benchmark', 'training']         1.0           1\n",
       "          abs-2211.12870v2  ActMAD: Activation Matching to Align Distributions for\\n  Test-Time-Training                       Test-Time-Training (TTT) is an approach to cope with out-of-distribution\\n(OOD) data by adapting a trained model to distribution shifts occurring at\\ntest-time. We propose to perform this adaptation via Activation Matching\\n(ActMAD): We analyze activations of the model and align activation statistics\\nof the OOD test data to those of the training data. In contrast to existing\\nmethods, which model the distribution of entire channels in the ultimate layer\\nof the feature extractor, we model the distribution of each feature in multiple\\nlayers across the network. This results in a more fine-grained supervision and\\nmakes ActMAD attain state of the art performance on CIFAR-100C and Imagenet-C.\\nActMAD is also architecture- and task-agnostic, which lets us go beyond image\\nclassification, and score 15.4% improvement over previous approaches when\\nevaluating a KITTI-trained object detector on KITTI-Fog. Our experiments\\nhighlight that ActMAD can be applied to online adaptation in realistic\\nscenarios, requiring little data to attain its full performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [('imagenet', 0.4636), ('trained', 0.3543), ('adaptation', 0.3414), ('activations', 0.3305), ('training', 0.3269)]        ['imagenet', 'trained', 'adaptation', 'activations', 'training']        1.0           1\n",
       "          abs-2308.03097v1  Incorporating Pre-training Data Matters in Unsupervised Domain\\n  Adaptation                       Unsupervised domain adaptation(UDA) and Source-free UDA(SFUDA) methods\\nformulate the problem involving two domains: source and target. They typically\\nemploy a standard training approach that begins with models pre-trained on\\nlarge-scale datasets e.g., ImageNet, while rarely discussing its effect.\\nRecognizing this gap, we investigate the following research questions: (1) What\\nis the correlation among ImageNet, the source, and the target domain? (2) How\\ndoes pre-training on ImageNet influence the target risk? To answer the first\\nquestion, we empirically observed an interesting Spontaneous Pulling (SP)\\nEffect in fine-tuning where the discrepancies between any two of the three\\ndomains (ImageNet, Source, Target) decrease but at the cost of the impaired\\nsemantic structure of the pre-train domain. For the second question, we put\\nforward a theory to explain SP and quantify that the target risk is bound by\\ngradient disparities among the three domains. Our observations reveal a key\\nlimitation of existing methods: it hinders the adaptation performance if the\\nsemantic cluster structure of the pre-train dataset (i.e.ImageNet) is impaired.\\nTo address it, we incorporate ImageNet as the third domain and redefine the\\nUDA/SFUDA as a three-player game. Specifically, inspired by the theory and\\nempirical findings, we present a novel framework termed TriDA which\\nadditionally preserves the semantic structure of the pre-train dataset during\\nfine-tuning. Experimental results demonstrate that it achieves state-of-the-art\\nperformance across various UDA and SFUDA benchmarks.                           [('adaptation', 0.41), ('imagenet', 0.3905), ('trained', 0.3732), ('training', 0.3192), ('risk', 0.2981)]                 ['adaptation', 'imagenet', 'trained', 'training', 'risk']               1.0           1\n",
       "Name: count, Length: 85939, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_result.groupby(['topic_id']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e3fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
